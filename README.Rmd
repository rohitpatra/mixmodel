---
title: "Estimation of a Two-component Mixture Model with Applications to Multiple Testing."
author: "Patra, R. K."
output: github_document
citecolor: blue
urlcolor: red
bibliography: SigNoise.bib

---

In this article, we discuss the computation of the confidence lower bound (see Section 4) and estimator (see Section 3) developed in @ps16. We also discuss the appropriate choice for the tuning parameter involved.

To download the R package use the following in R:

```{r setup}
library(devtools)
devtools::install_github(repo = "rohitpatra/mixmodel")
library(mixmodel)
```



We first generate an i.i.d sample of size $n= 5000$ from $$
F = \alpha_0 * \text{Beta}(1,10) + (1- \alpha_0 ) \text{Unif}(0,1),
$$ where $\alpha_0 =.1.$
```{r data_gen}
data.gen<- function( n, alpha){
	ind<- rbinom(n, 1, alpha)
	data<- ind* rbeta(n, 1,5) + (1-ind)* runif(n,0,1)
	return(data)
}
data.1<- data.gen(n = 500, alpha = .1)
```
For all computation in the above mixture model, we will use the R function named *mix.model*. Different values for the variable *method* lead to computation of different quantities.

### Lower confidence bound  for $\alpha_0$
The following code computes the $95\%$ confidence lower bound for $\alpha_0$
```{r lwr_bnd}
est.lwr.bnd <- mix.model(data.1, method = "lwr.bnd", gridsize = 600)
print(est.lwr.bnd)
plot(est.lwr.bnd)
```

### Estimation of $\alpha_0$

#### Pre-specified choice of $c_n$
The following code computes an estimate of $\alpha_0$ using the default choice of the tuning parameter, i.e., $c_n = 0.1 \log(\log(n))$

```{r fixed_c_n}
est.default <- mix.model(data.1, method = "fixed", gridsize = 600)
print(est.default)
plot(est.default)
```

The follwoing code gives an estimator of $\alpha_0$ when $c_n$ is chosen to be $0.05 \log\log(n)$
```{r eval=FALSE}
est.fixed <- mix.model(data.1, method = "fixed", c.n = .05*log(log(length(data.1))), gridsize = 600)
print(est.fixed)
plot(est.fixed)
```

#### Cross validated  choice of $c_n$
We can use the same function as above to compute the estimate of $\alpha_0$ when $c_n$ is chosen via cross-validation. The only difference being we have set *method* to be *cv*.

```{r eval=TRUE}
est.cv <- mix.model(data.1, method = "cv", gridsize = 600)
print(est.cv)
plot(est.cv)
```


## References
